\chapter{Diseño e implementación} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

Este capítulo aborda el proceso de construcción del sistema de recomendación, desde la concepción de la solución hasta su materialización en un entorno operativo. Se presentan las principales decisiones de diseño, el tratamiento de los datos y las técnicas empleadas para generar las recomendaciones, así como los lineamientos seguidos para asegurar que la propuesta resulte escalable, reproducible y alineada con los objetivos del negocio.

%----------------------------------------------------------------------------------------

\section{Diseño de solución}

El sistema de recomendación se diseñó con el objetivo de estimar la afinidad entre clientes y productos en un entorno caracterizado por alta escala, heterogeneidad de perfiles y rotación constante del portafolio. El diseño de la solución se apoyó en una arquitectura en capas que permite integrar diversas fuentes de datos, transformarlas en estructuras analíticas consistentes, entrenar modelos capaces de capturar relaciones complejas y, finalmente, desplegar las recomendaciones en un flujo operativo estable y reproducible.

La primera capa corresponde a la ingesta de datos, instancia en la que se integran registros transaccionales, interacciones digitales y atributos contextuales. Los datos transaccionales reflejan las compras efectivas realizadas en distintos horizontes temporales, lo que aporta evidencia directa sobre las preferencias observadas. Las interacciones digitales, en cambio, ofrecen señales implícitas de interés a partir de búsquedas, visualizaciones de productos o modificaciones en el carrito. Finalmente, los atributos contextuales caracterizan tanto a los clientes, mediante variables asociadas a su canal de comercialización, localización o tamaño, como a los productos, a partir de propiedades como marca, categoría o segmento.

La segunda capa se orienta a la preparación de los datos. En esta etapa se construyen las matrices de interacciones cliente–producto y se generan representaciones temporales que permiten capturar la dinámica de la demanda. Asimismo, se aplican técnicas de tratamiento de valores faltantes y de codificación de atributos categóricos, con el fin de asegurar consistencia y compatibilidad entre las distintas fuentes. El resultado de este proceso es un conjunto de estructuras homogéneas que sientan las bases para la etapa de modelado.

El modelado constituye la tercera capa de la arquitectura. En este punto se combinan distintos enfoques con el fin de maximizar la capacidad predictiva y superar las limitaciones de cada técnica individual. El filtrado colaborativo implícito, implementado a través de factorización matricial con el método ALS, permite capturar patrones latentes a partir de historiales de compra extensos. Los modelos basados en contenido complementan este enfoque al aprovechar descripciones de clientes y productos, y ofrecen una alternativa frente al problema del arranque en frío. Adicionalmente, se exploran modelos híbridos y de aprendizaje profundo capaces de integrar simultáneamente señales transaccionales y digitales, y de modelar relaciones no lineales entre las variables.

Finalmente, la capa de despliegue asegura la integración del motor de recomendación en el ecosistema tecnológico de la empresa. El \textit{pipeline} resultante genera listas de productos priorizados para cada cliente, incorpora mecanismos de versionado y monitoreo de modelos, y permite evaluar su desempeño en forma continua. De este modo, la solución se diseñó no solo para alcanzar precisión en la generación de recomendaciones, sino también para garantizar escalabilidad, reproducibilidad y adaptabilidad frente a la evolución del portafolio y a los cambios en los objetivos estratégicos.

La arquitectura de la solución se representa en la figura \ref{fig:arquitectura}. Allí se observa el flujo general del sistema, desde la integración de datos hasta la generación de recomendaciones. El diagrama sintetiza los módulos principales y sus interacciones, y ofrece una visión global que facilita comprender cómo se organiza el motor de afinidad.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Figures/Arquitectura.png}
  \caption{Arquitectura de alto nivel del sistema de recomendación.}
  \label{fig:arquitectura}
\end{figure}

%----------------------------------------------------------------------------------------

\section{Análisis exploratorio de los datos}

El análisis exploratorio constituye una etapa fundamental para comprender la estructura y los patrones subyacentes en la información disponible antes de su utilización en modelos de recomendación. Su objetivo es identificar distribuciones, tendencias y relaciones entre variables que permitan caracterizar el comportamiento de los clientes y del portafolio de productos, así como anticipar posibles limitaciones o sesgos que afecten el desempeño de los algoritmos.

En esta sección se examinan distintas dimensiones de los datos, e incluye la concentración de clientes y productos, la diversidad de los portafolios de compra, las correlaciones entre variables transaccionales y digitales, y la presencia de sesgos asociados a la popularidad. Este análisis preliminar no solo proporciona una visión descriptiva del conjunto de datos, sino que también orienta decisiones posteriores de ingeniería de atributos y diseño de modelos, al revelar qué señales resultan más informativas y qué fenómenos requieren un tratamiento específico.

\subsection{Curvas de concentración de clientes y productos}

El análisis de concentración constituye un paso clave para comprender la distribución del consumo en entornos de negocio masivo. La figura \ref{fig:concentracion_productos} muestra la curva de concentración de productos, donde se observa que un reducido conjunto concentra la mayor parte del volumen total. En particular, el 20\% de los productos explica cerca del 90\% de las ventas acumuladas, mientras que el resto conforma una extensa cola larga con niveles de rotación significativamente menores. Este comportamiento coincide con la ley de Pareto o principio 80/20 \cite{BOOK:Koch1998}, ampliamente documentado en mercados de consumo masivo, donde la dinámica competitiva se organiza en torno a un pequeño núcleo de artículos de alta popularidad y una mayoría de baja incidencia \cite{BOOK:Anderson2006}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.55]{./Figures/concentracion_productos.png}
	\caption{Concentración de productos en el portafolio.}
	\label{fig:concentracion_productos}
\end{figure}

De manera análoga, la figura \ref{fig:concentracion_clientes} refleja la concentración del consumo en la base de clientes. Los resultados indican que cerca del 20\% de los puntos de venta generan alrededor del 80\% del volumen total, lo que pone de manifiesto la existencia de clientes estratégicos que concentran gran parte de la demanda. Esta distribución desigual plantea desafíos relevantes para el diseño de sistemas de recomendación, ya que las señales provenientes de clientes de alto volumen tienden a dominar los modelos, lo que genera sesgos hacia productos y comportamientos mayoristas.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.55]{./Figures/concentracion_clientes.png}
	\caption{Concentración de clientes.}
	\label{fig:concentracion_clientes}
\end{figure}

La evidencia empírica confirma así que tanto el portafolio de productos como la base de clientes presentan fuertes patrones de concentración. En consecuencia, un motor de recomendación que busque maximizar su impacto no solo debe capturar la afinidad entre clientes y productos más relevantes, sino también considerar mecanismos que favorezcan la diversidad y la exploración de la cola larga. Esta perspectiva resulta fundamental para equilibrar la explotación de los artículos de mayor rotación con la exposición de productos menos populares, lo que alinea los objetivos de negocio con la mejora de la experiencia del cliente.

\subsection{Patrones de diversidad en el portafolio}

El análisis de la diversidad en el portafolio de productos por cliente permite comprender la amplitud y heterogeneidad de los hábitos de consumo. La figura \ref{fig:hist_diversidad} muestra la distribución del número de productos distintos adquiridos por cliente en un mes. Los resultados evidencian que la mayoría de los puntos de venta concentra su demanda en un conjunto reducido de referencias, mientras que un número menor incorpora una mayor amplitud de marcas y presentaciones. Esta asimetría confirma la coexistencia de clientes de bajo rango de exploración con otros de portafolio más diversificado.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.5]{./Figures/hist_diversidad.png}
	\caption{Histograma de diversidad de portafolio: número de productos distintos por cliente.}
	\label{fig:hist_diversidad}
\end{figure}

% Las diferencias se acentúan al segmentar por canal comercial, como se puede apreciar en la figura \ref{fig:boxplot_diversidad}. En este caso, se observa que los autoservicios tienden a manejar un surtido más amplio de productos en comparación con kioscos y tiendas tradicionales, lo que refleja el rol que cada formato cumple dentro de la red de distribución. Este hallazgo es consistente con la literatura en consumo masivo, que indica que la variedad de portafolio suele estar asociada a factores estructurales como el tamaño del punto de venta y la frecuencia de reposición \cite{BOOK:Kotler2017}.

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[scale=.6]{./Figures/boxplot_diversidad_canal.png}
% 	\caption{Diversidad de portafolio segmentada por canal comercial.}
% 	\label{fig:boxplot_diversidad}
% \end{figure}

La figura \ref{fig:log_log_plot} ilustra el fenómeno de concentración extrema en la demanda, donde unos pocos productos acumulan la mayoría de los pedidos mientras que la gran mayoría registra volúmenes marginales. Para representar este patrón se utiliza un \textit{log–log plot}, en el cual tanto el ranking de los productos como su número total de pedidos se expresan en escala logarítmica. Esta transformación permite visualizar con mayor claridad distribuciones de tipo cola larga, que en escalas lineales suelen quedar ocultas por la presencia de artículos extremadamente populares. El gráfico muestra una pendiente decreciente que confirma la existencia de este comportamiento: un reducido conjunto de productos concentra un volumen muy elevado, mientras que el resto se distribuye en la larga cola de baja rotación.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.5]{./Figures/log_log_plot.png}
	\caption{\textit{Log log plot} de la popularidad de productos.}
	\label{fig:log_log_plot}
\end{figure}

Este patrón no solo refuerza la evidencia presentada en las curvas de concentración, sino que además resalta un sesgo estructural que enfrenta cualquier sistema de recomendación en entornos de consumo masivo. Al entrenarse sobre datos históricos, los modelos tienden de manera natural a privilegiar los productos más populares, lo que reproduce el sesgo de popularidad y reduce la diversidad de las sugerencias. Este fenómeno señala la tensión entre explotación de productos estrella y exploración de la cola larga \cite{BOOK:Celma2010}. En este contexto, el desafío consiste en diseñar mecanismos que permitan balancear ambos extremos, de modo que se garantice relevancia sin sacrificar diversidad ni cobertura.

El análisis de co-ocurrencia entre los productos más relevantes, presentado en la figura \ref{fig:heatmap_coocurrencia}, revela patrones de complementariedad en la demanda. Determinadas marcas y presentaciones tienden a aparecer de manera conjunta en los carritos de compra, lo que sugiere asociaciones naturales que pueden ser aprovechadas por un motor de recomendación. Estos resultados refuerzan la importancia de capturar no solo la popularidad individual de cada producto, sino también las relaciones de afinidad que emergen a nivel de portafolio.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.4]{./Figures/heatmap_coocurrencia.png}
	\caption{Mapa de calor de co-ocurrencia entre los 10 productos más relevantes.}
	\label{fig:heatmap_coocurrencia}
\end{figure}

\subsection{Correlaciones entre variables transaccionales y digitales}

El análisis de correlaciones busca identificar hasta qué punto las señales digitales anticipan comportamientos de compra y, en consecuencia, evaluar su potencial como insumos predictivos. Con el fin de evaluar la relación entre interacciones digitales y transacciones, se construyó una matriz de correlación entre las principales variables del conjunto de datos, observada en la figura \ref{fig:heatmap_corr}. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=.5]{./Figures/heatmap_corr.png}
	\caption{Matriz de correlación entre variables transaccionales y digitales.}
	\label{fig:heatmap_corr}
\end{figure}

Los resultados muestran una correlación elevada entre \texttt{ORDERED} y \texttt{BUYER} ($r=0.70$), coherente con el hecho de que ambas variables reflejan distintos aspectos de la misma dimensión de compra. En contraste, las correlaciones de las señales digitales con las variables de compra resultan positivas pero de menor magnitud: \texttt{CARD\_VIEWED} y \texttt{DETAILS\_PAGE\_VIEWED} muestran coeficientes bajos, lo que indica que la exposición y exploración de productos acompaña el proceso de compra, aunque no lo determina. La variable \texttt{REMOVED} presenta la relación más débil, lo que sugiere que los eventos de descarte contienen información ruidosa y limitada respecto de la propensión a comprar.

La correlación contemporánea entre interacciones digitales y compras confirma que las transacciones pasadas siguen siendo el principal indicador de comportamiento, mientras que las señales digitales aportan evidencia complementaria que, si bien débil de manera aislada, resulta relevante al integrarse en un modelo híbrido.

Con el fin de explorar la capacidad predictiva de estas variables, se calculó la correlación de cada una con la compra del mismo cliente–producto en el mes siguiente. Los resultados, en la figura \ref{fig:corr_prox_mes},  muestran que las transacciones pasadas (\texttt{BUYER}, \texttt{ORDERED}) son los predictores más fuertes, aunque las señales digitales también aportan información incremental. En particular, la variable \texttt{CARD\_VIEWED} presenta un coeficiente relevante, lo que respalda la hipótesis de que la exposición reiterada a un producto incrementa la probabilidad de recompra.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.6]{./Figures/corr_prox_mes.png}
	\caption{Correlación de variables con la compra en el mes siguiente.}
	\label{fig:corr_prox_mes}
\end{figure}

Finalmente, se evaluó la tasa de recompra según la combinación de señales observadas en meses previos, en la tabla \ref{tab:tasa_recompra}. Los clientes que registran tanto interacción como transacción presentan la mayor tasa de recompra (58,8\%), seguidos por aquellos con solo órdenes (44,1\%). En contraste, quienes solo exhiben interacciones digitales alcanzan un nivel considerablemente menor (17,6\%), incluso por debajo del grupo sin ningún registro previo (28,3\%). Este resultado sugiere que las interacciones aisladas no constituyen un predictor confiable de recompra, sino que tienden a reflejar un interés superficial que rara vez se traduce en pedidos. En cambio, la combinación de transacciones previas con señales digitales se confirma como el escenario de mayor poder explicativo, ya que aprovecha la solidez de la evidencia transaccional y, al mismo tiempo, permite mejorar la capacidad de anticipar comportamientos futuros en casos donde no existen registros abundantes de compra.

\begin{table}[H]
	\centering
	\caption[Tasa de recompra por combinación de señales]{Tasa de recompra según la combinación de señales previas.}
	\label{tab:tasa_recompra}
	\begin{tabular}{l c}    
		\toprule
		\textbf{Grupo}     & \textbf{Tasa de recompra}\\
		\midrule
		Interacción y orden & 58{,}79 \% \\
		Solo orden & 44{,}05 \% \\
		Ninguno & 28{,}30 \% \\
		Solo interacción & 17{,}64 \% \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Observaciones preliminares del análisis exploratorio}

El análisis exploratorio permitió identificar una serie de patrones que resultan fundamentales para orientar el diseño del motor de recomendación. En primer lugar, se confirmó que tanto el portafolio de productos como la base de clientes presentan fuertes niveles de concentración: un reducido conjunto explica la mayor parte del volumen, mientras que la mayoría se distribuye en una extensa cola larga de baja rotación. Este comportamiento introduce un sesgo hacia popularidad que los modelos deben manejar para no sacrificar diversidad \cite{BOOK:Anderson2006}.

En segundo lugar, se observó que la diversidad en los portafolios de compra varía según el tipo de cliente, con autoservicios que incorporan un surtido más amplio en comparación con kioscos y tiendas tradicionales. Además, los análisis de co-ocurrencia revelaron asociaciones frecuentes entre ciertos productos, lo que sugiere la existencia de complementariedades que pueden ser aprovechadas en la generación de recomendaciones.  

En tercer lugar, el estudio de correlaciones entre variables digitales y transaccionales mostró que, si bien las transacciones pasadas constituyen el predictor más sólido de comportamiento, las señales digitales aportan información incremental y se vuelven especialmente relevantes en escenarios de arranque en frío. La evaluación de tasas de recompra confirmó que la combinación de interacciones y compras pasadas es la fuente más robusta de predicción, mientras que las interacciones aisladas presentan un valor explicativo limitado.  

En conjunto, estos hallazgos proporcionan una primera validación de la hipótesis central: la integración de señales transaccionales y digitales, complementadas con atributos contextuales, resulta clave para capturar la heterogeneidad del consumo y diseñar un motor de recomendación capaz de balancear precisión, diversidad y cobertura.

%----------------------------------------------------------------------------------------

\section{Preparación e ingeniería de los datos}

La preparación e ingeniería de los datos constituyó uno de los pilares centrales del desarrollo del motor de afinidad, al definir cómo las distintas fuentes de información fueron transformadas en insumos consistentes, comparables y numéricamente útiles para el entrenamiento de los modelos de recomendación. El proceso integró fuentes transaccionales, digitales y contextuales, y abordó tanto la construcción de la matriz cliente–producto como la generación de atributos descriptivos de clientes y productos.

\subsection{Fuentes y estructura general}

El punto de partida son tres conjuntos principales de información: registros transaccionales, eventos digitales generados en la aplicación BEES y atributos contextuales de clientes y productos. Esta combinación permite construir una representación integral de la relación cliente–producto, en la que se entrelazan tanto preferencias explícitas como señales implícitas de interés.

Los registros transaccionales se encuentran a nivel de operación individual, con granularidad diaria y asociados a identificadores de cliente, producto, cantidad y monto. Como fue previsto en el análisis exploratorio, estos datos suelen exhibir un fuerte patrón de concentración cercano a la regla de Pareto \cite{BOOK:Koch1998}, donde una pequeña fracción de marcas o ítems concentra la mayor parte del volumen. Este fenómeno, común en la industria, anticipa la necesidad de mitigar los sesgos hacia productos de alta rotación durante el modelado.

Los eventos digitales, por su parte, contienen información de búsquedas, visualizaciones, adiciones y remociones en el carrito, así como clics en promociones. Estas interacciones permiten capturar señales tempranas de interés que no siempre se traducen en compras efectivas, pero amplían la cobertura del sistema. No obstante, su naturaleza exploratoria introduce ruido, por lo que se aplican filtros para eliminar registros residuales o no representativos, y se priorizan solo aquellos que expresan comportamientos consistentes de interés \cite{BOOK:Ricci2015,ARTICLE:Zhang2019}.

Finalmente, los atributos contextuales ofrecen información complementaria sobre las características estructurales de clientes y productos. En los primeros, se incluyen variables de canal, localización y tamaño del punto de venta; en los segundos, descriptores como marca, segmento, envase o unidad de negocio. Estos factores resultan esenciales para capturar la heterogeneidad comercial que no siempre se refleja en los registros transaccionales.

\subsection{Construcción de la matriz cliente-producto}

La matriz cliente–producto constituye el núcleo del sistema de recomendación, al representar de forma estructurada las interacciones históricas y digitales entre los puntos de venta y el portafolio. Su construcción requirió integrar los eventos válidos depurados, agregados a nivel mensual por cliente y producto, con la contabilización de la frecuencia de ocurrencia de cuatro tipos principales: \texttt{ORDERED}, \texttt{CARD\_VIEWED}, \texttt{DETAILS\_PAGE\_VIEWED} y \texttt{REMOVED}. Los dos primeros incluyeron subtipos que reflejan el origen de la recomendación, como búsquedas populares o programas de fidelización, preservados por su relevancia estadística.

El diseño temporal siguió la lógica operativa del sistema: las predicciones correspondientes al mes $N$ se generan durante el mes $N-1$ a partir de la información disponible previamente. Por ello, se implementó un esquema de ventanas móviles de seis meses ($N-7$ a $N-2$) con horizontes de observación de uno, tres y seis meses. Este enfoque permite capturar simultáneamente señales recientes y patrones de comportamiento estables, lo que mantiene la coherencia temporal del modelo \cite{ARTICLE:Koren2010,BOOK:Aggarwal2016}.

El conjunto de variables presentaba una alta heterogeneidad en magnitudes, derivada de la coexistencia de clientes con volúmenes dispares y productos de distinta rotación. Para mitigar los efectos de escala, se aplicó \textit{winsorizing} (\textit{clipping}) sobre los percentiles extremos \cite{BOOK:Aggarwal2015} y se normalizaron las variables dentro del grupo cliente–categoría, de modo que cada valor representara la importancia relativa de un producto dentro del portafolio del cliente. 

A partir de estas variables se estimaron pesos específicos por tipo de evento y horizonte temporal, combinados para obtener un \textit{score} compuesto de preferencia implícita. Los pesos se ajustaron mediante optimización bayesiana con \texttt{Optuna} \cite{ARTICLE:Akiba2019}, con el objetivo de maximizar la métrica \textit{Precision@10} sobre validación temporal. Se observó un patrón consistente de mayor relevancia para señales recientes sobre las que incorporan historia más lejana y para eventos transaccionales respecto de los digitales. El resultado fue una matriz de afinidad que sintetiza 60,2 millones de pares cliente–producto, con una densidad del 13,5\%, una correlación Buyer–Preference de 0,39 y un AUC de 0,8766, lo que evidencia una alta capacidad discriminante (tabla~\ref{tab:metricas_matriz}).

\begin{table}[H]
	\centering
	\caption[Métricas descriptivas de la matriz cliente–producto]{Resumen de métricas descriptivas de la matriz cliente–producto.}
	\begin{tabular}{l c}    
		\toprule
		\textbf{Indicador}     & \textbf{Valor}\\
		\midrule
		Total de pares cliente–producto & 60 220 260 \\
		Densidad de la matriz (\% de celdas no nulas) & 13{,}50 \% \\
		Clientes en arranque en frío & 2 981 (1{,}50 \%) \\
		Productos en arranque en frío & 1 (0{,}54 \%) \\
		Media del \textit{score} de preferencia & 0{,}0046 \\
		Desvío estándar del \textit{score} & 0{,}0277 \\
		Correlación Buyer–Preference & 0{,}3935 \\
		Área bajo la curva (AUC) & 0{,}8766 \\
		\bottomrule
	\end{tabular}
	\label{tab:metricas_matriz}
\end{table}

\subsection{Diseño de atributos de cliente y producto}

Además del \textit{score} de interacción, se incorporaron atributos estructurales de clientes y productos para enriquecer las señales de afinidad. Para los puntos de venta se construyeron seis bloques principales: frecuencia de compra, estabilidad temporal, volumen y crecimiento, diversidad del mix, comportamiento de compra y atributos contextuales. En los productos, se modelaron variables de volumen, penetración, desempeño, diversidad geográfica y de canal, junto con descriptores estructurales como segmento y tipo de envase. 

Todas las variables numéricas fueron tratadas con \textit{winsorizing} y normalización \textit{z-score} por grupo de negocio o canal, seguidas de discretización en tres cuantiles (bajo, medio, alto). Las variables categóricas se estandarizaron y completaron con etiquetas neutras. De este modo, se obtuvieron representaciones comparables, robustas y fácilmente integrables en modelos que emplean \textit{feature embeddings}.

\subsubsection{Evaluación de representatividad}

Para evaluar la calidad informativa de los atributos, se analizó la varianza, la correlación con la ocurrencia de compra y la similitud geométrica entre entidades. En los clientes (95 atributos válidos), las mayores varianzas correspondieron a variables de volumen y frecuencia, mientras que las correlaciones más altas con la compra se asociaron con continuidad y madurez comercial, con coeficientes próximos a 0,32 y una similitud coseno promedio de 0,9985. En los productos (61 atributos), las correlaciones más fuertes (0,84) se observaron en penetración de clientes y cobertura comercial, con similitud coseno promedio de 0,64, lo que indica una mayor dispersión relativa del portafolio.

Complementariamente, un análisis de componentes principales (PCA) permitió visualizar la estructura de estas representaciones. En los clientes, la proyección reveló una diferenciación coherente entre los canales Autoservicio (AS) y Kiosco + Tradicional (K+T), mientras que en los productos emergieron grupos claramente definidos por unidad de negocio. Esto demuestra que las \textit{features} capturan patrones estructurales reales y diferencian adecuadamente la heterogeneidad comercial de la base (figuras~\ref{fig:pca_clientes} y~\ref{fig:pca_productos}).

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=0.7\textwidth]{pca_clientes.png}
% 	\caption[Proyección PCA de clientes por canal comercial]{Proyección PCA de clientes por canal comercial.}
% 	\label{fig:pca_clientes}
% \end{figure}


% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=0.7\textwidth]{pca_productos.png}
% 	\caption[Proyección PCA de productos por línea de negocio]{Proyección PCA de productos por línea de negocio.}
% 	\label{fig:pca_productos}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pca_clientes.png}
        \caption{Proyección PCA de clientes por canal comercial.}
        \label{fig:pca_clientes}
    \end{subfigure}
    \hspace{0.005\textwidth}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pca_productos.png}
        \caption{Proyección PCA de productos por línea de negocio.}
        \label{fig:pca_productos}
    \end{subfigure}
    \caption[Proyecciones PCA de clientes y productos]{Proyecciones PCA de clientes y productos, diferenciadas por canal comercial y línea de negocio, respectivamente.}
    \label{fig:pca_comparativo}
\end{figure}

La preparación e ingeniería de los datos permitió consolidar una base analítica sólida, donde la matriz cliente–producto captura las señales implícitas de afinidad y los atributos de cliente y producto aportan contexto y generalización, lo que habilita el uso combinado de enfoques colaborativos e híbridos en las etapas siguientes.


%----------------------------------------------------------------------------------------

\section{Desarrollo de modelos}

El desarrollo de los modelos constituye la fase central del sistema de recomendación, donde la matriz cliente–producto construida en etapas previas se transforma en un mecanismo capaz de estimar la afinidad entre ambos. El objetivo es asignar a cada combinación posible un puntaje continuo que refleje la probabilidad relativa de interés, lo que permite ordenar los productos según la relevancia esperada para cada cliente.

Para abordar este desafío se exploraron distintos enfoques de modelado, que combinan estrategias colaborativas, basadas en contenido y de aprendizaje profundo. En primer lugar, se implementó un modelo de filtrado colaborativo mediante el algoritmo \textit{Alternating Least Squares} \cite{ARTICLE:ALS2008, ARTICLE:Hu2008}, que aprende representaciones latentes de clientes y productos a partir de los patrones de interacción observados. En segundo lugar, se desarrolló un modelo híbrido con \textit{LightFM} \cite{ARTICLE:LightFM2015}, capaz de integrar señales colaborativas con atributos explícitos de clientes y productos, lo que mitiga el problema del arranque en frío.

Complementariamente, se incorporó una variante basada en redes neuronales del tipo \textit{Two Towers} \cite{ARTICLE:Yi2019}, que aprende representaciones de clientes y productos a partir de sus atributos categóricos y numéricos mediante arquitecturas de \textit{embeddings}. Sus representaciones pueden combinarse con las generadas por ALS en un esquema híbrido de ensamble, lo que fortalece la robustez y la capacidad de generalización del sistema. La segunda corresponde al enfoque de \textit{Neural Collaborative Filtering} \cite{ARTICLE:He2017}, que reemplaza la combinación lineal de \textit{embeddings} por un modelo neuronal capaz de capturar relaciones no lineales de afinidad.

En conjunto, estos modelos representan una progresión desde métodos clásicos hacia aproximaciones más flexibles y expresivas. Las siguientes subsecciones describen los fundamentos, la formulación y las particularidades de cada uno, lo que sienta las bases para la comparación de desempeño y costos que se desarrolla en el capítulo siguiente.

\subsection{Filtrado colaborativo con ALS}

El primer enfoque desarrollado fue un modelo de filtrado colaborativo basado en factorización matricial mediante el algoritmo \textit{Alternating Least Squares} \cite{ARTICLE:Koren2009}. Este método permite descomponer la matriz cliente–producto en dos espacios latentes de menor dimensión, uno para los clientes y otro para los productos, de modo que la afinidad entre ambos se estime como el producto interno de sus vectores representativos. El objetivo es capturar patrones de coocurrencia en las interacciones históricas y proyectarlos hacia combinaciones no observadas, lo que permite generar recomendaciones personalizadas a partir del comportamiento colectivo.

\subsubsection{Configuración del modelo}

En este trabajo se utilizó la variante de ALS para \textit{feedback} implícito, apropiada para contextos donde las señales de preferencia provienen de interacciones observadas en lugar de calificaciones explícitas \cite{ARTICLE:Hu2008}. Bajo este esquema, la ausencia de interacción no se interpreta como una valoración negativa, sino como falta de evidencia. La matriz de entrada corresponde al \textit{score} de preferencia compuesto descrito en la sección anterior, el que integra eventos transaccionales y digitales ponderados según su relevancia y temporalidad. Cada celda representa un grado de afinidad implícita entre el cliente y el producto, estimado a partir de seis meses de comportamiento histórico.

El modelo se configuró con un número de factores latentes ajustable (\textit{rank}), un parámetro de regularización $\lambda$ para controlar el sobreajuste y un parámetro de confianza $\alpha$ que pondera la influencia de las observaciones positivas frente a las ausentes \cite{ARTICLE:Hu2008}. El entrenamiento se realizó de manera iterativa, con la alternancia en la actualización de las matrices de clientes y productos hasta alcanzar la convergencia.

\subsubsection{Optimización de hiperparámetros}

Para la selección de hiperparámetros se implementó un proceso de optimización bayesiana con \texttt{Optuna} \cite{ARTICLE:Akiba2019}, compuesto por veinte iteraciones orientadas a maximizar la métrica \textit{Precision@5} sobre un conjunto de validación temporal. La búsqueda abarcó los principales parámetros del modelo: dimensión latente (\textit{rank} $\in [10, 50]$), número máximo de iteraciones (\texttt{maxIter} $\in [5, 20]$), nivel de regularización (\texttt{regParam} $\in [10^{-4}, 10^{-1}]$) y parámetro de confianza (\texttt{alpha} $\in [10^{-2}, 10]$). 

En cada iteración, el modelo fue entrenado sobre las interacciones comprendidas entre los meses $N-7$ y $N-2$, y evaluado sobre el mes $N-1$, lo que reprodujo el flujo real de generación de recomendaciones.

% \subsubsection{Configuración óptima obtenida}

% El proceso de optimización permitió identificar la siguiente configuración como la de mejor desempeño general, maximizando la precisión en los primeros resultados sin comprometer la estabilidad del entrenamiento:

% \begin{table}[H]
% 	\centering
% 	\caption[Hiperparámetros óptimos del modelo ALS]{Configuración óptima del modelo de filtrado colaborativo con \texttt{ALS} obtenida mediante optimización bayesiana.}
% 	\begin{tabular}{l c}
% 		\toprule
% 		\textbf{Parámetro} & \textbf{Valor óptimo} \\
% 		\midrule
% 		Dimensión latente (\texttt{rank}) & 26 \\
% 		Regularización (\texttt{regParam}) & 0{,}0979 \\
% 		Iteraciones máximas (\texttt{maxIter}) & 11 \\
% 		Restricción de no negatividad (\texttt{nonnegative}) & False \\
% 		Bloques de usuario (\texttt{numUserBlocks}) & 10 \\
% 		Bloques de ítem (\texttt{numItemBlocks}) & 10 \\
% 		Feedback implícito (\texttt{implicitPrefs}) & True \\
% 		Tamaño de bloque (\texttt{blockSize}) & 4096 \\
% 		Parámetro de confianza (\texttt{alpha}) & 0{,}5415 \\
% 		Intervalo de checkpoint (\texttt{checkpointInterval}) & 10 \\
% 		Estrategia de arranque en frío (\texttt{coldStartStrategy}) & \texttt{drop} \\
% 		\bottomrule
% 	\end{tabular}
% 	\label{tab:best_params_als}
% \end{table}

\subsubsection{Resultados y conclusiones}

Los resultados obtenidos mostraron un comportamiento consistente y robusto. El modelo alcanzó una \textit{Precision@10} de 31{,}5\% y una \textit{Recall@10} de 33{,}7\%, lo que indica una buena capacidad para priorizar los productos efectivamente comprados en el siguiente período. Se observó además que el ALS tiende a capturar de manera eficiente las relaciones entre clientes con historial suficiente, pero su desempeño disminuye en escenarios de arranque en frío o cuando la matriz presenta elevada dispersión. Por ello, este modelo se adoptó como línea base sobre la que se construyeron enfoques híbridos más expresivos en las siguientes etapas.

El modelo ALS demostró ser una herramienta eficaz para extraer representaciones latentes de afinidad a partir del comportamiento histórico \cite{ARTICLE:Koren2009}. Su estructura matemática simple, estabilidad en entornos de gran escala y adecuación al feedback implícito lo convierten en un componente esencial del \textit{pipeline} de recomendación desarrollado.

\subsection{Modelo híbrido con LightFM}

El segundo enfoque explorado fue un modelo híbrido basado en la biblioteca \textit{LightFM} \cite{ARTICLE:LightFM2015}, que combina técnicas de filtrado colaborativo y modelos basados en contenido dentro de un mismo marco de aprendizaje. Este modelo extiende la factorización matricial tradicional al incorporar vectores de características (\textit{feature embeddings}) asociados tanto a los usuarios como a los ítems, lo que permite capturar información contextual incluso para aquellos pares que no poseen interacciones históricas, lo que mitiga el problema de arranque en frío identificado en la sección anterior.

A diferencia del ALS, que aprende representaciones exclusivamente a partir de la matriz de interacciones, LightFM incorpora atributos estructurales de clientes y productos como señales adicionales \cite{ARTICLE:LightFM2015}. En este trabajo, las representaciones de cliente incluyeron variables de frecuencia, estabilidad, volumen, mezcla, comportamiento y contexto, mientras que las de producto comprendieron características de volumen, penetración, composición de canal, segmento, diversidad y desempeño. Cada conjunto de variables fue normalizado, discretizado y codificado antes de su integración al modelo, conforme al \textit{pipeline} de ingeniería descrito previamente.

El modelo \textit{LightFM} combina señales colaborativas y de contenido dentro de un espacio latente común, y representa tanto a los clientes como a los productos mediante vectores que integran información transaccional y contextual. Su entrenamiento parte de una matriz binaria de interacciones, donde cada par cliente–producto indica la existencia o no de contacto en el período de análisis. Estas interacciones positivas constituyen la evidencia de afinidad sobre la que el modelo aprende a distinguir entre ítems relevantes y no relevantes para cada usuario.

A diferencia del enfoque de ALS, que busca ajustar magnitudes continuas de preferencia implícita, \textit{LightFM} se entrena mediante la optimización de una función de pérdida orientada al ordenamiento relativo de los ítems en el ranking. Este tipo de funciones, ampliamente utilizadas en escenarios de \textit{feedback} implícito, penalizan los errores en las primeras posiciones y favorecen que los productos con mayor probabilidad de interacción ocupen los primeros lugares en las recomendaciones.

A cada interacción se le asignó además un peso relativo o \textit{sample weight}, que determina su influencia durante el entrenamiento. Estos pesos se derivaron del puntaje de preferencia implícito calculado en la etapa de ingeniería de atributos y se transformaron mediante una función logarítmica que reduce la dispersión entre observaciones extremas. Posteriormente, los valores fueron normalizados alrededor de su media global, de manera que las interacciones más intensas, como compras frecuentes o múltiples eventos asociados al mismo producto, tuvieran mayor impacto que aquellas esporádicas.  

Este esquema permite que el modelo capture no sólo la ocurrencia de una interacción, sino también su intensidad relativa, ya que integra señales de distinta fuerza en el proceso de aprendizaje. A diferencia de ALS, que optimiza una función cuadrática ponderada con el objetivo de reconstruir las magnitudes observadas de preferencia implícita, \textit{LightFM} utiliza un criterio de aprendizaje basado en el ordenamiento, donde los pesos asignados a cada interacción afectan directamente la probabilidad de que un ítem sea priorizado en el ranking final \cite{ARTICLE:Rendle2009, ARTICLE:LightFM2015}. Esto permite capturar diferencias más finas en la intensidad de las señales, ya que integra tanto la frecuencia como la relevancia relativa de cada evento dentro del proceso de entrenamiento.

El entrenamiento se llevó a cabo sobre el conjunto de interacciones ponderadas, con el empleo de las matrices de características de usuario y producto generadas en la etapa anterior. El modelo se optimizó durante veinte épocas, mediante el uso de procesamiento paralelo en cuatro hilos, hasta alcanzar estabilidad en la función objetivo. Esta configuración resultó adecuada para equilibrar precisión y eficiencia computacional, y sirvió como base para los distintos ensayos de complejidad incremental desarrollados a continuación.

\subsubsection{Evaluación experimental del modelo LightFM}

Con el objetivo de analizar el aporte incremental de las variables contextuales, se realizaron tres configuraciones experimentales del modelo \textit{LightFM} con distintos niveles de complejidad en las representaciones de usuario y producto.  
Se describen los detalles metodológicos de cada ensayo, así como la selección de variables, la arquitectura del modelo y los criterios de evaluación. El detalle completo de la experimentación se presenta en el Anexo~\ref{AnexoLightFMTests}.

Los resultados mostraron una mejora progresiva en la capacidad predictiva al incorporar variables discretizadas y explicativas, seguida de una estabilización del desempeño tras la depuración final de atributos. Las métricas globales se mantuvieron estables y competitivas, con valores de \textit{Precision@10} y \textit{Recall@10} cercanos al 26\,\% y 28\,\%, respectivamente (tabla~\ref{tab:resultados_lightfm_a}).

\begin{table}[H]
	\centering
	\caption[Resultados comparativos de los ensayos con LightFM]{Resumen de métricas de desempeño para las tres configuraciones experimentales del modelo \textit{LightFM}.}
	\begin{tabular}{l c c}    
		\toprule
		\textbf{Configuración} & \textbf{Precision@10} & \textbf{Recall@10} \\
		\midrule
		Test 1 – Contexto categórico reducido & 25{,}3 \% & 27{,}1 \% \\
		Test 2 – Atributos discretizados y explicativos & 26{,}1 \% & 27{,}8 \% \\
		Test 3 – Depuración y selección de \textit{embeddings} & 25{,}8 \% & 27{,}5 \% \\
		\bottomrule
	\end{tabular}
	\label{tab:resultados_lightfm_a}
\end{table}

Estos resultados confirman que el modelo híbrido logra capturar relaciones no lineales y de alta dimensionalidad en entornos de señales implícitas dispersas, lo que valida la coherencia de las representaciones latentes y establece una base sólida para la versión final optimizada que se presenta en la siguiente sección.

\subsubsection{Optimización bayesiana de hiperparámetros}

Con el objetivo de maximizar la precisión del ranking y validar la robustez del modelo frente a diferentes configuraciones, se llevó a cabo un proceso de optimización bayesiana mediante la biblioteca \texttt{Optuna}. La búsqueda se orientó a maximizar la métrica \textit{Precision@5}, con prioridad en la capacidad del sistema para ubicar los productos más relevantes en las primeras posiciones de recomendación.

El espacio de búsqueda incluyó combinaciones de número de componentes latentes (\texttt{no\_components} $\in \{32, 64, 96, 128, 192\}$), funciones de pérdida \texttt{WARP} y \texttt{BPR}, tasas de aprendizaje en el rango $[5 \times 10^{-4}, 5 \times 10^{-3}]$, y parámetros de regularización \texttt{item\_alpha} y \texttt{user\_alpha} en el intervalo $[10^{-6}, 10^{-4}]$. También se evaluaron valores de \texttt{max\_sampled} entre 5 y 15, con el fin de lograr un balance adecuado entre exploración y costo computacional.

% El proceso permitió identificar la configuración con mejor desempeño general, manteniendo la estabilidad del entrenamiento y optimizando la calidad del ranking en escenarios reales de recomendación. La configuración óptima encontrada se detalla en la tabla \ref{tab:lightfm_best_params}.

% \begin{table}[H]
% 	\centering
% 	\caption[Hiperparámetros óptimos del modelo LightFM]{Configuración óptima del modelo híbrido con \texttt{LightFM} obtenida mediante optimización bayesiana.}
% 	\begin{tabular}{l c}
% 		\toprule
% 		\textbf{Parámetro} & \textbf{Valor óptimo} \\
% 		\midrule
% 		Función de pérdida (\texttt{loss}) & \texttt{bpr} \\
% 		Dimensión latente (\texttt{no\_components}) & 96 \\
% 		Tasa de aprendizaje (\texttt{learning\_rate}) & 0{,}00489 \\
% 		Regularización de ítems (\texttt{item\_alpha}) & $3{,}48 \times 10^{-5}$ \\
% 		Regularización de usuarios (\texttt{user\_alpha}) & $2{,}23 \times 10^{-6}$ \\
% 		Muestras negativas máximas (\texttt{max\_sampled}) & 8 \\
% 		Algoritmo de optimización (\texttt{learning\_schedule}) & \texttt{adagrad} \\
% 		\bottomrule
% 	\end{tabular}
% 	\label{tab:lightfm_best_params}
% \end{table}

\subsubsection{Resultados y conclusiones}

El modelo alcanzó una \textit{Precision@10} de 29{,}2\% y un \textit{Recall@10} de 31{,}2\%, superó las versiones anteriores tanto en precisión como en cobertura, y se consolidó como la mejor alternativa dentro del conjunto de modelos evaluados.

Estos resultados confirman que la combinación de la función de pérdida \texttt{BPR} con un número intermedio de componentes latentes ofrece un equilibrio óptimo entre capacidad representacional y generalización, lo que maximiza la recuperación de productos relevantes sin sobreajustar a las interacciones más frecuentes. El modelo resultante constituye la versión final del motor híbrido de afinidad, el cual integra señales transaccionales y contextuales dentro de un espacio latente de alta coherencia semántica.

\subsection{Modelo basado en contenido con arquitectura Two-Tower}

El modelo \textit{Two-Tower} representa una extensión moderna del enfoque híbrido introducido con \textit{LightFM}, y se caracteriza por la incorporación de una red neuronal de dos torres entrenada para aprender representaciones continuas (\textit{embeddings}) de clientes y productos a partir de sus características estructurales \cite{ARTICLE:Huang2013}.  

A diferencia de \textit{LightFM}, que combina señales colaborativas y de contenido en una factorización lineal, la arquitectura \textit{Two-Tower} permite capturar interacciones no lineales de alta dimensionalidad mediante capas densas y funciones de activación, lo que otorga mayor poder expresivo y capacidad de generalización en escenarios complejos. Mientras \textit{LightFM} aprende relaciones principalmente lineales entre las \textit{features} y las preferencias, el modelo \textit{Two-Tower} introduce una parametrización no lineal capaz de capturar interacciones complejas entre variables numéricas y categóricas.  

Cada torre de la red, una para clientes y otra para productos, procesa sus respectivos vectores de características contextuales y numéricas, y genera un espacio latente común donde las entidades con patrones de comportamiento o atributos similares quedan próximas entre sí \cite{ARTICLE:Huang2013,WEB:TFR2021}. El modelo se entrena con el objetivo de maximizar la similitud coseno entre pares positivos (cliente–producto con interacción) y de minimizarla frente a un conjunto de ejemplos negativos muestreados por contexto (\textit{hard negatives}) \cite{ARTICLE:Yi2019}. 
Esta estrategia de entrenamiento orientada al ranking, basada en la combinación de pérdidas \textit{MarginRankingLoss} y \textit{Binary Cross–Entropy}, permite equilibrar la discriminación entre ítems relevantes y la estabilidad del aprendizaje.

\subsubsection{Diseño y configuración del modelo}

El conjunto de características de entrada se definió a partir de los atributos estructurales más representativos de clientes y productos utilizados en los modelos anteriores.  

El modelo se configuró con un tamaño de \textit{embedding} de 128 dimensiones y dos capas ocultas de 128 y 64 neuronas en cada torre, conectadas mediante funciones de activación \texttt{ReLU}. La figura~\ref{fig:two_tower} muestra la arquitectura general del modelo, donde ambas torres, una para los clientes y otra para los productos, aprenden representaciones independientes que luego se combinan mediante una medida de similitud en el espacio latente para estimar la afinidad entre ambas entidades.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.3]{./Figures/two_tower_2.png}
	\caption{Arquitectura del modelo \textit{Two Towers}\protect\footnotemark.}
	\label{fig:two_tower}
\end{figure}

\footnotetext{Imagen tomada de \url{https://arxiv.org/pdf/1708.05031} \cite{ARTICLE:He2017}}

El entrenamiento se llevó a cabo durante cinco épocas con un tamaño de lote de 1024 y una tasa de aprendizaje de 0,001, con el uso del optimizador \texttt{Adam}.
Esta configuración equilibró adecuadamente la capacidad representacional y la estabilidad del aprendizaje, lo que mantuvo un costo computacional razonable para los volúmenes de datos involucrados.

Un aspecto central del proceso de entrenamiento fue la generación de ejemplos negativos (\textit{negative sampling}) para complementar los pares positivos de cliente y producto observados \cite{ARTICLE:Yi2019}.  
Por cada interacción positiva se generaron ocho combinaciones negativas, seleccionadas de forma controlada dentro del mismo contexto de negocio o segmento del producto. A diferencia de un muestreo aleatorio, que puede introducir casos triviales o poco informativos, este enfoque de \textit{hard negative sampling} busca construir ejemplos desafiantes, es decir, productos similares pero no adquiridos por el cliente. De esta manera, el modelo aprende a distinguir entre afinidades reales y coincidencias superficiales, lo que refuerza su capacidad discriminante y la calidad de las representaciones en el espacio latente.

\subsubsection{Ensamble híbrido con ALS}

Con el objetivo de integrar señales de naturaleza distinta, colaborativas y de contenido, se implementó un ensamble entre el modelo ALS y el \textit{Two-Tower}.  
Ambos generan puntuaciones continuas de afinidad sobre el espacio cliente–producto, las que fueron combinadas mediante una función de promedio ponderado, representado en la ecuación \ref{eq:ensamble}, donde $\lambda$ controla el peso relativo de la componente colaborativa (ALS) y de la componente de contenido (\textit{Two-Tower}).

\begin{equation}
\label{eq:ensamble}
\text{Score}_{hybrid} = \lambda \cdot \text{Score}_{ALS} + (1 - \lambda) \cdot \text{Score}_{TT}
\end{equation}
 
El valor óptimo de $\lambda$ se ajustó empíricamente en base a la métrica \textit{Precision@10}, observándose que ponderaciones de \(\lambda = 0{,}8\) lograron el mejor equilibrio entre precisión y cobertura.  

Este enfoque híbrido aprovecha simultáneamente la densidad informativa de las interacciones históricas y la riqueza contextual de las representaciones aprendidas por el modelo neuronal, lo que consolida un sistema de recomendación más robusto frente a la escasez de datos y la variabilidad estructural del portafolio.

\subsubsection{Resultados y conclusiones}

El modelo \texttt{Two-Tower} mostró un desempeño superior respecto de los enfoques previos, lo que consolidó su efectividad para capturar relaciones no lineales entre clientes y productos a partir de atributos contextuales y numéricos.
En la evaluación sobre el conjunto de validación, alcanzó una \textit{Precision@10} de 32{,}3\% y un \textit{Recall@10} de 34{,}5\%, y superó tanto al modelo ALS como a las variantes híbridas de \textit{LightFM} en ambas métricas.

Estos resultados reflejan la capacidad del modelo para generar representaciones continuas de mayor expresividad y generalización, integrando de manera efectiva las señales de contexto estructural, la intensidad de las interacciones y las relaciones latentes aprendidas en los espacios de \textit{embedding}.

\subsection{\textit{Neural Collaborative Filtering}}

El modelo \textit{Neural Collaborative Filtering (NCF)} constituye una evolución directa de los enfoques basados en factorización latente, al reemplazar la combinación lineal de \textit{embedding} por una arquitectura neuronal capaz de modelar interacciones no lineales entre usuarios y productos \cite{ARTICLE:He2017}.  
Este modelo integra dos componentes complementarios: una capa \textit{Generalized Matrix Factorization} (GMF) que conserva la naturaleza lineal y explicativa del filtrado colaborativo clásico, y una red \textit{Multi-Layer Perceptron} (MLP) que aprende patrones de interacción de mayor complejidad.  

La fusión de ambas salidas en la capa final, denominada \textit{NeuMF}, permite capturar simultáneamente señales de proximidad latente y relaciones no lineales de alto orden, lo que extiende la capacidad de generalización del sistema de recomendación.

En comparación con el modelo \textit{Two-Tower}, que entrena torres independientes a partir de características contextuales, el NCF opera directamente sobre los identificadores embebidos de usuarios y productos, y modela de forma explícita las interacciones entre ambos espacios latentes.
Esta característica introduce un valor adicional en escenarios de datos implícitos, ya que permite aprender una función de similitud más expresiva sin que dependa de features contextuales externas, y aprovecha las señales puramente colaborativas del histórico transaccional.

\subsubsection{Diseño y configuración}

El modelo se implementó bajo la arquitectura \textit{NeuMF}, que combina dos componentes complementarios en paralelo: un bloque GMF, responsable de modelar las interacciones multiplicativas lineales mediante \textit{embeddings} de 64 dimensiones, y un bloque MLP que incorpora una parametrización no lineal del mismo tamaño de \textit{embedding}, seguido por dos capas densas de 128 y 64 neuronas con activación \texttt{ReLU} y regularización \texttt{Dropout(0.1)}.  
Ambas representaciones se integran en una capa de fusión que concatena las salidas de los dos caminos y las proyecta en una capa final lineal con activación sigmoide, encargada de estimar la probabilidad de interacción positiva entre cada par usuario–producto.  
La figura~\ref{fig:ncf_hibrido} ilustra la estructura híbrida del modelo, y muestra cómo las ramas GMF y MLP convergen en una representación conjunta que combina el poder predictivo de la factorización lineal con la expresividad de las redes neuronales profundas.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.25]{./Figures/ncf_hibrido.png}
	\caption{Arquitectura del modelo \textit{Neural Collaborative Filtering} híbrido\protect\footnotemark.}
	\label{fig:ncf_hibrido}
\end{figure}

\footnotetext{Imagen tomada de \url{https://arxiv.org/pdf/1708.05031} \cite{ARTICLE:He2017}}

El entrenamiento se realizó durante cinco épocas con un tamaño de lote de 4096 y una tasa de aprendizaje de 0,001.  
Por cada interacción positiva observada, se generaron cuatro negativos contextuales seleccionados dentro del mismo segmento de negocio y familia de producto.  
Este procedimiento, conocido como \textit{hard negative sampling}, busca aumentar la dificultad de aprendizaje del modelo al exponerlo a ejemplos negativos más informativos y semánticamente próximos, lo que fortalece la capacidad de discriminación del clasificador.

\subsubsection{Resultados y conclusiones}

El modelo NCF alcanzó una \textit{Precision@10} de 32{,}4\% y un \textit{Recall@10} de 34{,}3\% sobre el conjunto de validación.  
Estos resultados evidencian una mejora significativa respecto de los modelos puramente basados en factorización o redes separadas, lo que demuestra la efectividad del enfoque híbrido \texttt{GMF + MLP} para capturar tanto relaciones lineales como no lineales entre las entidades.

Además de su mayor capacidad predictiva, el modelo presentó una convergencia estable y un comportamiento robusto frente a distintas configuraciones de negativos y tamaños de \textit{embedding}, lo que lo convierte en una alternativa eficiente para escenarios de datos altamente implícitos.  

En conjunto, el \textit{Neural Collaborative Filtering} representa el paso más avanzado dentro del \textit{pipeline} de afinidad, lo que consolida una arquitectura neuronal completamente diferenciable y optimizable de extremo a extremo, que integra de manera orgánica los principios de factorización matricial y aprendizaje profundo.

%----------------------------------------------------------------------------------------

\section{Implementación}

La implementación del sistema de recomendación requirió articular los distintos componentes desarrollados dentro de un flujo de trabajo unificado, reproducible y escalable. Para ello se diseñó un \textit{pipeline} modular que integra los procesos de ingesta, transformación, modelado y evaluación, con soporte para el versionado y monitoreo de artefactos en producción.

\subsection{Diseño del \textit{pipeline} de procesamiento}

El flujo completo se estructuró en cuatro etapas principales: ingesta, preparación, modelado y predicción.  
En la fase de ingesta se integraron las fuentes de datos transaccionales, digitales y contextuales en un entorno distribuido, lo que garantizó la consistencia de los identificadores y la alineación temporal entre registros.

Durante la preparación, se aplicaron las transformaciones de limpieza, agregación, codificación y normalización para construir la matriz cliente–producto que sirve de insumo a los modelos.

En la etapa de modelado, se ejecutaron los distintos enfoques desarrollados, y se almacenaron sus métricas, parámetros y versiones.

Finalmente, en la fase de predicción se generaron los puntajes de afinidad y las listas \textit{Top-$K$} para cada cliente, que constituyen la salida principal del sistema.

\subsection{Integración con la infraestructura tecnológica}

La ejecución del \textit{pipeline} se realizó en la plataforma Databricks \cite{ARTICLE:Databricks}, que permitió procesar grandes volúmenes de datos de forma distribuida mediante el uso de PySpark. Este entorno facilitó la orquestación de tareas, la paralelización de los cálculos y la trazabilidad de los resultados.  

Para la gestión del ciclo de vida de los modelos se empleó MLflow \cite{ARTICLE:MLflow2018}, herramienta que permitió registrar los experimentos, almacenar los parámetros y métricas, y versionar los artefactos generados durante el entrenamiento. Cada ejecución de modelo quedó asociada a un identificador único, lo que posibilita reproducir resultados, comparar configuraciones y recuperar versiones históricas de los modelos entrenados.  

Esta integración entre Databricks y MLflow conformó una infraestructura robusta y escalable, adecuada tanto para la experimentación iterativa como para la implementación de \textit{pipelines} automatizados.

\subsection{Estrategias de versionado y monitoreo}

Con el fin de garantizar la trazabilidad del sistema, se adoptaron prácticas de control de versiones y monitoreo continuo.  

El código fuente y los scripts asociados al \textit{pipeline} se gestionaron mediante GitHub \cite{ARTICLE:GitHub}, lo que permitió organizar el desarrollo de manera colaborativa y mantener un historial de cambios documentado.  

Por otro lado, los modelos registrados en MLflow se acompañaron de sus métricas de validación y fecha de generación, lo que posibilitó un seguimiento temporal de su desempeño.

Además, se establecieron controles de consistencia sobre los datos de entrada y validaciones automáticas del formato de salida, lo que aseguró la estabilidad operativa del sistema en cada ejecución.

En conjunto, esta arquitectura permitió implementar un flujo de trabajo integrado, auditable y escalable, lo que garantizó la reproducibilidad de los resultados y sentó las bases para la futura incorporación de componentes en producción.


% \definecolor{mygreen}{rgb}{0,0.6,0}
% \definecolor{mygray}{rgb}{0.5,0.5,0.5}
% \definecolor{mymauve}{rgb}{0.58,0,0.82}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % parámetros para configurar el formato del código en los entornos lstlisting
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \lstset{ %
%   backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
%   basicstyle=\footnotesize,        % the size of the fonts that are used for the code
%   breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
%   breaklines=true,                 % sets automatic line breaking
%   captionpos=b,                    % sets the caption-position to bottom
%   commentstyle=\color{mygreen},    % comment style
%   deletekeywords={...},            % if you want to delete keywords from the given language
%   %escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
%   %extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
%   %frame=single,	                % adds a frame around the code
%   keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
%   keywordstyle=\color{blue},       % keyword style
%   language=[ANSI]C,                % the language of the code
%   %otherkeywords={*,...},           % if you want to add more keywords to the set
%   numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
%   numbersep=5pt,                   % how far the line-numbers are from the code
%   numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
%   rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
%   showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
%   showstringspaces=false,          % underline spaces within strings only
%   showtabs=false,                  % show tabs within strings adding particular underscores
%   stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
%   stringstyle=\color{mymauve},     % string literal style
%   tabsize=2,	                   % sets default tabsize to 2 spaces
%   title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
%   morecomment=[s]{/*}{*/}
% }


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
% \section{Análisis del software}
 
% La idea de esta sección es resaltar los problemas encontrados, los criterios utilizados y la justificación de las decisiones que se hayan tomado.

% Se puede agregar código o pseudocódigo dentro de un entorno lstlisting con el siguiente código:

% \begin{verbatim}
% \begin{lstlisting}[caption= "un epígrafe descriptivo"]
% 	las líneas de código irían aquí...
% \end{lstlisting}
% \end{verbatim}

% A modo de ejemplo:

% \begin{lstlisting}[label=cod:vControl,caption=Pseudocódigo del lazo principal de control.]  % Start your code-block

% #define MAX_SENSOR_NUMBER 3
% #define MAX_ALARM_NUMBER  6
% #define MAX_ACTUATOR_NUMBER 6

% uint32_t sensorValue[MAX_SENSOR_NUMBER];		
% FunctionalState alarmControl[MAX_ALARM_NUMBER];	//ENABLE or DISABLE
% state_t alarmState[MAX_ALARM_NUMBER];						//ON or OFF
% state_t actuatorState[MAX_ACTUATOR_NUMBER];			//ON or OFF

% void vControl() {

% 	initGlobalVariables();
	
% 	period = 500 ms;
		
% 	while(1) {

% 		ticks = xTaskGetTickCount();
		
% 		updateSensors();
		
% 		updateAlarms();
		
% 		controlActuators();
		
% 		vTaskDelayUntil(&ticks, period);
% 	}
% }
% \end{lstlisting}



